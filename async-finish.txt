[[ch:async-finish]]
== Chapter: Structured Parallelism with Async-Finish

The "async-finish" approach offers another mechanism for structured
parallelism.  It is similar to fork-join parallelism but is more
flexible, because it allows essentially any number of parallel
branches to synchronize at the same point called a "finish".  Each
"async" creates a separate parallel branch, which by construction must
signal a "finish."

As with fork-join, it is often possible to extend an existing language
with support for async-finish parallelism by providing libraries or
compiler extensions that support a few simple primitives.  Such
extensions to a language make it easy to derive a sequential program
from a parallel program by syntactically substituting the parallelism
annotations with corresponding serial annotations.  This in turn
enables reasoning about the semantics or the meaning of parallel
programs by essentially "ignoring" parallelism, e.g., via the
sequential elision mechanism.


=== Parallel Fibonacci via Async-Finish

Recall that the latexmath:[$n^{th}$] Fibonnacci number is defined
by the recurrence relation

[latexmath]
++++
$$
\begin{array}{lcl}
F(n) & = & F(n-1) + F(n-2)
\end{array}
$$
++++

with base cases

[latexmath]
++++
$$
F(0) = 0, \, F(1) = 1
$$
++++

To write a parallel version, we remark that the two recursive calls
are completely *_independent_*: they do not depend on each other
(neither uses a piece of data generated or written by another).  We
can therefore perform the recursive calls in parallel.  In general,
any two independent functions can be run in parallel.  To indicate
that two functions can be run in parallel, we use `async()`.  It is a
requirement that each `async` takes place in the context of a
`finish`.  All `async`'ed computations that takes place in the context
of a `finish` synchronize at that `finish`, i.e., `finish` completes
only when all the `async`'ed computations complete.  The important
point about a `finish` is that it can serve as the synchronization
point for an arbitrary number of `async`'ed computations.  This is not
quite useful in Fibonacci because there are only two computations to
synchronize at a time, but  it will be useful in our next example.

[source, {cpp}]
----
long fib_par(long n, long &result) {
  if (n < 2) {
    *result = n;
  } else {
    long a, b;
    finish {
      async [&] fib_par(n-1, a);
      aysnc [&] fib_par(n-2, b);
    });
    *result = *a + *b;
  }
}
----

=== Incrementing an array, in parallel

Recall our example for mapping an array to another by incrementing
each element by one.  We can write the code for a function `map_incr`
that performs this computation serially.

[source,{cpp}]
----
void map_incr(const long* source, long* dest, long n) {
  for (long i = 0; i < n; i++)
    dest[i] = source[i] + 1;
}
----

Using async-finish, we can write to code parallel array increment by
using only a single `finish` block and `async`' ing all the parallel
computations inside that `finish`.

[source,{cpp}]
----
void map_incr_rec_aux (const long* source, long* dest, long lo, long hi) {
  long n = hi - lo;
  if (n == 0) {
    // do nothing
  } else if (n == 1) {
    dest[lo] = source[lo] + 1;
  } else {
    long mid = (lo + hi) / 2;
    async ([&] {
      map_incr_rec_aux(source, dest, lo, mid);
    };
    async [&] {
      map_incr_rec_aux(source, dest, mid, hi);
    };
  }
}


void map_incr_rec (const long* source, long* dest, long lo, long hi) {

  finish ([&] { 
    map_inc_rec_aux (source, dest, lo, hi));
  };
} 

----

It is helpful to compare this to fork-join, where we had to
synchronize parallel branches in pairs.  Here, we are able to
synchronize them all at a single point.


.Dag for parallel increment on an array of latexmath:[$8$] using async-finish: Each vertex corresponds a call to `map_inc_rec_aux` excluding the `async`  or the continuation of `async`, which is empty, and is annotated with the interval of the input array that it operates on (its argument).
[[fig:parallel-inc-dag-async-finish]]
image::jpgs-620H/inc-parallel-dag-async-finish.jpg["Dag for parallel increment using async-finish",width="500pt",align="center"]

<<fig:parallel-inc-dag-async-finish, The Figure above>> illustrates
the dag for an execution of `map_incr_rec`.  Each invocation of this
function corresponds to a thread labeled by "M".  The threads labeled
by latexmath:[$M\lbrack i,j \rbrack $] corresponds to the part of the
invocation of `map_incr_rec_aux` with arguments `lo` and `hi` set to
latexmath:[$i$] and latexmath:[$j$] respectively.  Note that all
threads synchronize at the single finish node.

Based on this dag, we can create another dag, where each thread is
replaced by the sequence of instructions that it represents.  This
would give us a picture similar to the
<<multithreading::general-threads, dag we drew before>> for general
multithreaded programs.  Such a dag representation, where we represent
each instruction by a vertex, gives us a direct way to calculate the
work and span of the computation.  If we want to calculate work and
span on the dag of threads, we can label each vertex with a weight
that corresponds to the number of instruction in that thread.

Using async-finish does not alter the asymptotic work and span of the
computation compared to fork-join, which remain as latexmath:[O(n)]
work and latexmath:[$O(\log{n})$] span.  In practice, however, the
async-finish version creates fewer threads (by about a factor of two),
which can make a difference.
