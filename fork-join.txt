[[ch:fork-join]]
== Chapter: Fork-join parallelism

Fork-join parallelism, a fundamental model in parallel computing,
dates back to 1963 and has since been widely used in parallel
computing. In fork join parallelism, computations create opportunities
for parallelism by branching at certain points that are specified by
annotations in the program text.

Each branching point *_forks_* the control flow of the computation
into two or more logical threads. When control reaches the branching
point, the branches start running. When all branches complete, the
control *_joins_* back to unify the flows from the branches. Results
computed by the branches are typically read from memory and merged at
the join point. Parallel regions can fork and join recursively in the
same manner that divide and conquer programs split and join
recursively. In this sense, fork join is the divide and conquer of
parallel computing.


As we will see, it is often possible to extend an existing language
with support for fork-join parallelism by providing libraries or
compiler extensions that support a few simple primitives.  Such
extensions to a language make it easy to derive a sequential program
from a parallel program by syntactically substituting the parallelism
annotations with corresponding serial annotations.  This in turn
enables reasoning about the semantics or the meaning of parallel
programs by essentially "ignoring" parallelism. 


PASL is a Clatexmath:[++] library that enables writing implicitly parallel
programs.  In PASL, fork join is expressed by application of the
`fork2()` function. The function expects two arguments: one for each
of the two branches. Each branch is specified by one Clatexmath:[++]
lambda expression.

.Fork join
==========================

In the sample code below, the first branch writes the value 1 into the
cell `b1` and the second 2 into `b2`; at the join point, the sum of
the contents of `b1` and `b2` is written into the cell `j`.

[source,{cpp}]
----
long b1 = 0;
long b2 = 0;
long j  = 0;

fork2([&] {
  // first branch
  b1 = 1;
}, [&] {
  // second branch
  b2 = 2;
});
// join point
j = b1 + b2;

std::cout << "b1 = " << b1 << "; b2 = " << b2 << "; ";
std::cout << "j = " << j << ";" << std::endl;
----

Output:

----
b1 = 1; b2 = 2; j = 3;
----

==========================

When this code runs, the two branches of the fork join are both run to
completion. The branches may or may not run in parallel (i.e., on
different cores). In general, the choice of whether or not any two
such branches are run in parallel is chosen by the PASL runtime
system. The join point is scheduled to run by the PASL runtime only
after both branches complete. Before both branches complete, the join
point is effectively blocked. Later, we will explain in some more
detail the scheduling algorithms that the PASL uses to handle such
load balancing and synchronization duties.

In fork-join programs, a thread is a sequence of instructions that do
not contain calls to `fork2()`.  A thread is essentially a piece of
sequential computation.  The two branches passed to `fork2()` in the
example above correspond, for example, to two independent
threads. Moreover, the statement following the join point (i.e., the
continuation) is also a thread.

NOTE: If the syntax in the code above is unfamiliar, it might be a
good idea to review the notes on lambda expressions in Clatexmath:[++]11.  In a
nutshell, the two branches of `fork2()` are provided as
lambda-expressions where all free variables are passed by reference.

NOTE: Fork join of arbitrary arity is readily derived by repeated
application of binary fork join. As such, binary fork join is
universal because it is powerful enough to generalize to fork join of
arbitrary arity.

All writes performed by the branches of the binary fork join are
guaranteed by the PASL runtime to commit all of the changes that they
make to memory before the join statement runs. In terms of our code
snippet, all writes performed by two branches of `fork2` are committed
to memory before the join point is scheduled. The PASL runtime
guarantees this property by using a local barrier. Such barriers are
efficient, because they involve just a single dynamic synchronization
point between at most two processors.




.Writes and the join statement
==========================

In the example below, both writes into `b1` and `b2` are guaranteed to
be performed before the print statement.

[source,{cpp}]
----
long b1 = 0;
long b2 = 0;

fork2([&] {
  b1 = 1;
}, [&] {
  b2 = 2;
});

std::cout << "b1 = " << b1 << "; b2 = " << b2 << std::endl;
----

Output:
----
b1 = 1; b2 = 2
----

PASL provides no guarantee on the visibility of writes between any two
parallel branches. In the code just above, for example, writes
performed by the first branch (e.g., the write to `b1`) may or may not
be visible to the second, and vice versa. 

==========================


=== Parallel Fibonacci

Now, we have all the tools we need to describe our first parallel
code: the recursive Fibonacci function. Although useless as a program
because of efficiency issues, this example is the "hello world" program
of parallel computing.

Recall that the latexmath:[$n^{th}$] Fibonnacci number is defined
by the recurrence relation

[latexmath]
++++
$$
\begin{array}{lcl}
F(n) & = & F(n-1) + F(n-2)
\end{array}
$$
++++

with base cases

[latexmath]
++++
$$
F(0) = 0, \, F(1) = 1
$$
++++

Let us start by considering a sequential algorithm. Following the
definition of Fibonacci numbers, we can write the code for
(inefficiently) computing the latexmath:[$n^{th}$] Fibonnacci number
as follows. This function for computing the Fibonacci numbers is
inefficient because the algorithm takes exponential time, whereas
there exist dynamic programming solutions that take linear time.

//////////////////////////
WARNING: Naama: there are also sequential algorithms that can caluclate Fib(n) in log(n) time (using matrix multiplication).
////////////////////////


[source, {cpp}]
----
long fib_seq (long n) {
  long result;
  if (n < 2) {
    result = n;
  } else {
    long a, b;
    a = fib_seq(n-1);
    b = fib_seq(n-2);
    result = a + b;
  }
  return result;
}
----

To write a parallel version, we remark that the two recursive calls
are completely *_independent_*: they do not depend on each other
(neither uses a piece of data generated or written by another).  We
can therefore perform the recursive calls in parallel.  In general,
any two independent functions can be run in parallel.  To indicate
that two functions can be run in parallel, we use `fork2()`.

[source, {cpp}]
----
long fib_par(long n) {
  long result;
  if (n < 2) {
    result = n;
  } else {
    long a, b;
    fork2([&] {
      a = fib_par(n-1);
    }, [&] {
      b = fib_par(n-2);
    });
    result = a + b;
  }
  return result;
}
----


=== Incrementing an array, in parallel


Suppose that we wish to map an array to another by incrementing each
element by one.  We can write the code for a function `map_incr` that
performs this computation serially.

[source,{cpp}]
----
void map_incr(const long* source, long* dest, long n) {
  for (long i = 0; i < n; i++)
    dest[i] = source[i] + 1;
}
----

.Example: Using `map_incr`.
=============
The code below illustrates  an example use of `map_incr`.

[source,{cpp}]
----
const long n = 4;
long xs[n] = { 1, 2, 3, 4 };
long ys[n];
map_incr(xs, ys, n);
for (long i = 0; i < n; i++)
  std::cout << ys[i] << " ";
std::cout << std::endl;
----

Output:

----
2 3 4 5 
----

=============

This is not a good parallel algorithm but it is not difficult to give
a parallel algorithm for incrementing an array. The code for such an
algorithm is given below.


[source,{cpp}]
----
void map_incr_rec(const long* source, long* dest, long lo, long hi) {
  long n = hi - lo;
  if (n == 0) {
    // do nothing
  } else if (n == 1) {
    dest[lo] = source[lo] + 1;
  } else {
    long mid = (lo + hi) / 2;
    fork2([&] {
      map_incr_rec(source, dest, lo, mid);
    }, [&] {
      map_incr_rec(source, dest, mid, hi);
    });
  }
}
----

It is easy to see that this algorithm has latexmath:[O(n)] work and
latexmath:[$O(\log{n})$] span.  


=== The sequential elision

In the Fibonacci example, we started with a sequential algorithm and
derived a parallel algorithm by annotating independent functions.  It
is also possible to go the other way and derive a sequential algorithm
from a parallel one.  As you have probably guessed this direction is
easier, because all we have to do is remove the calls to the `fork2`
function. The sequential elision of our parallel Fibonacci code can be
written by replacing the call to `fork2()` with a statement that
performs the two calls (arguments of `fork2()`) sequentially as
follows.

[source, {cpp}]
----
long fib_par(long n) {
  long result;
  if (n < 2) {
    result = n;
  } else {
    long a, b;
    ([&] {
      a = fib_par(n-1);
    })();
    ([&] {
      b = fib_par(n-2);
    })();
    result = a + b;
  }
  return result;
}
----

NOTE: Although this code is slightly different than the sequential
version that we wrote, it is not too far away, because the only the
difference is the creation and application of the lambda-expressions.
An optimizing compiler for Clatexmath:[++] can easily "inline" such
computations. Indeed, After an optimizing compiler applies certain
optimizations, the performance of this code the same as the
performance of `fib_seq`.


The sequential elision is often useful for debugging and for
optimization.  It is useful for debugging because it is usually easier
to find bugs in sequential runs of parallel code than in parallel runs
of the same code.  It is useful in optimization because the
sequentialized code helps us to isolate the purely algorithmic
overheads that are introduced by parallelism. By isolating these
costs, we can more effectively pinpoint inefficiencies in our code.


===  Executing fork-join algorithms

We defined fork-join programs as a subclass case of multithreaded
programs.  Let's see more precisely how we can "map" a fork-join
program to a multithreaded program. An our running example, let's use the
`map_incr_rec`, whose code is reproduced below.

[source,{cpp}]
----
void map_incr_rec(const long* source, long* dest, long lo, long hi) {
  long n = hi - lo;
  if (n == 0) {
    // do nothing
  } else if (n == 1) {
    dest[lo] = source[lo] + 1;
  } else {
    long mid = (lo + hi) / 2;
    fork2([&] {
      map_incr_rec(source, dest, lo, mid);
    }, [&] {
      map_incr_rec(source, dest, mid, hi);
    });
  }
}
----

The key question is this: what is a thread?  Clearly, when we write a
multithreaded program, we don't manipulate explicitly threads. The
basic idea is to partition a computation, which is a run of a parallel
algorithm on a specified input, into pieces of serial computations,
and define these pieces as threads.

We call a piece of serial computation a *_thread_*, if it executes
 without performing parallel operations (`fork2`) except perhaps as
 its last action.  The term thread is short for *_user-level thread_*
 (as opposed to a operating-system thread).  When partitioning the
 computation into threads, it is important for threads to be maximal;
 technically a thread can be as small as a single instruction.

.Definition: Thread
***********

A *thread* is a maximal computation consisting of a sequence of
instructions that do not contain calls to `fork2()` except perhaps at
the very end.

***********


.DAG for parallel increment on an array of latexmath:[$8$]: Each vertex corresponds a call to `map_inc_rec` excluding the `fork2`  or the continuation of `fork2`, which is empty, an is annotated with the interval of the input array that it operates on (its argument).
[[fig:parallel-inc-dag]]
image::inc-parallel-dag.png["DAG for parallel increment",width="800pt",align="center"]

<<fig:parallel-inc-dag, The Figure above>> illustrates the DAG for an execution
of `map_incr_rec`.  We partition each invocation of this function into
two threads labeled by "M" and "C" respectively.  The threads labeled
by latexmath:[$M[i,j]$] corresponds to the part of the invocation of
`map_incr_rec` with arguments `lo` and `hi` set to latexmath:[$i$] and
latexmath:[$j$] respectively; this first part corresponds to the part
of execution up and including the `fork2` or all of the function if
this is a base case.  The second corresponds to the "continuation" of
the `fork2`, which is in this case includes no computation.

Based on this dag, we can create another dag, wherea each thread is
replaced by the sequence of instructions that it represents.  This
would give us a picture similar to the
<<multithreading::general-threads, dag we drew before>> for general
multithreaded programs.  Such a dag representation, where we represent
each instruction by a vertex, gives us a direct way to calculate the
work and span of the computation.  If we want to calculate work and
span ond the dag of threads, we can label each vertex with a weight
that corresponds to the number of instruction in that thread.



Having talked about DAG's we are now ready to talk about how to map
parallel computations to actual hardware so as to minimize their
run-time, i.e., scheduling. 

 But before we move on to scheduling let
us observe a few properties of implicitly parallel computations.

. The computation DAG of a parallel algorithm applied to an input
unfolds dynamically as the algorithm executes. For example, when we
run `map_inc_rec` with an input with latexmath:[$n$] elements, the DAG
initially contains just the root vertex (thread) corresponding to the
first call to `map_inc_rec` but it grows as the execution proceeds.

. An execution of a parallel algorithm can generate a massive number
of threads.  For example, our `map_inc_rec' function generates
approximately latexmath:[$4n$] threads for an input with
latexmath:[$n$] elements.

. The work/span of each thread can vary from a small amount to a very
large amount depending on the algorithm.  In our example, each thread
performs either a conditional, sometimes an addition and a fork
operation or performs no actual computation (continuation threads).

Suppose now we are given a computation DAG and we wish to execute the
DAG by mapping each thread to one of the latexmath:[$P$] processor that
is available on the hardware. When a thread is mapped to a processor,
it will be executed requiring time proportional to the work (weight)
of the thread.  Once the processor completes the thread, we can map
another thread to the processor, so that the processor does not idle
unnecessarily.  As we map threads to processors, it is important that
we observe the dependencies between threads, i.e., we don't execute a
thread before its parents. 



, and map them to available
processors while observing the dependencies between them. The task of
mapping the threads to available processors is called *_thread
scheduling_* or simply *_scheduling_*.

When scheduling a parallel computation, it is important that we don't
alter the intended meaning of the computation.  Specifically, if a
thread depends another thread, because for example, it reads a piece
of data generated by the latter, it cannot be executed before the
thread that it depends on.  We can conservatively approximate such
dependencies by observing the `fork2` expressions and by organizing
dependencies consistently with them.  More specifically, we can
represent a computations as a graph where each vertex represents a
thread and each edge represents a dependency.  Vertices and edges are
created by execution of `fork2`.  Each `fork2` creates two threads
(vertices) corresponding to the two branches and inserts an edge
between each branch and the thread that performs the `fork2` branches;
in addition, each `fork2` creates a join or continuation thread
(vertex) that depends on the two branches. Since such a graph cannot
contain cycles, it is a *_Directed Acyclic Graph (DAG)_*.



.Definition: Scheduling
************

The (thread) scheduling problem requires assigning to each thread in a
given DAG a processor number and a time step such that

. each thread is assigned to a unique processor for as many
consecutive steps as its weight,

. no thread is executed before its descendants in the DAG, and

. no processor is assigned more at most one thread at a time.

************

The goal of scheduling to minimize critical resources such as time.
Computing the shortest schedule for a DAG turns out to be highly
nontrivial. In fact, the related decision problem in NP-complete.  It
is possible, however, to give a good approximation algorithm for the
offline version of the problem to generate a 2-factor approximation.
Such an appraximation yields a schedule for a given DAG within a
factor-two of the shortest schedule.  In the online version of the
problem, where the DAG unfolds as the computation executes, we don't
know the DAG a priori and we have to account for the costs for
scheduling such as migrating threads between schedulers and finding
work.  To execute parallel programs, we need an solution to this
online version of the problem.

An online scheduler or a simply a *_scheduler_* is an algorithm that
performs scheduling by mapping threads to available processors.  For
example, if only one processor is available, a scheduler can map all
threads to that one processor.  If two processors are available, then
the scheduler can divide the threads between the two processors as
evenly as possible in an attempt to keep the two processors as busy
as possible by *_load balancing_*.

.An example 2-processor schedule
==========

The following is a valid schedule for the DAG shown in
<<fig:parallel-inc-dag, this Figure>> assuming that each thread takes
unit time.

[width="100%",frame="topbot",options="header"]
|==========
|Time Step | Processor 1               | Processor 2
|1         | M [0,8)                   | 
|2         | M [0,4)                   | M [4,8)
|3         | M [0,2)                   | M [4,6)
|4         | M [0,1)                   | M [4,5)
|5         | M [1,2)                   | M [5,6)
|6         | C [0,2)                   | C [4,6)
|7         | M [2,4)                   | M [6,8)
|8         | M [2,3)                   | M [6,7)
|9         | M [3,4)                   | M [7,8)
|10        | C [2,4)                   | C [6,8)
|11        | C [0,4)                   | C [4,8)
|12        | C [0,8)                   | _
|==========
==========



We say that a scheduler is *_greedy_* if, whenever there is a
processor available and a thread ready to be executed, then the
scheduler assigns the thread to the processor and starts running the
thread immediately.  Greedy schedulers have a nice property that is
summarized by the following theorem.


[[thm:greedy-scheduling]]
.Theorem: Greedy Scheduling Principle
***********

If a computation is run on latexmath:[$P$] processors using a perfect
greedy scheduler that incurs no costs in creating, locating, and
moving threads, then the total time (clock cycles) for running the
computation latexmath:[$T_P$] is bounded by

[latexmath]
++++

$$
T_P  <  \frac{W}{p} + S.
$$

++++

Here latexmath:[$W$] is the work of the computation, and
latexmath:[$S$] is the span of the computation (both measured in units
of clock cycles).

****


This simple statement is powerful.  To see this, note that the time to
execute the computation is at least latexmath:[$\frac{W}{P}$] because
we have a total of latexmath:[$W$] work. As such, the best possible
execution strategy is to divide it evenly among the processors.
Furthermore, execution time cannot be less than latexmath:[$S$] since
latexmath:[$S$] represents the longest chain of sequential
dependencies.  Thus we have: latexmath:[ $ T_P \geq
\max\left(\frac{W}{P},S\right).  $]

This means that a greedy schudeler yields a schedule that is within a
factor two of optimal: latexmath:[$\frac{W}{P} + S$] is never more
than twice latexmath:[$\max(\frac{W}{P},S)$]. Furthermore, when
latexmath:[$\frac{W}{P} \gg S$], the difference between the greedy
scheduler and the optimal scheduler is very small.  In fact, we can
rewrite equation above in terms of the average parallelism
latexmath:[$\mathbb{P} = W/S$] as follows:

[latexmath]
++++

$$
\begin{array}{rcl}
T_p & < & \frac{W}{P} + S \\
 & = & \frac{W}{P} + \frac{W}{\mathbb{P}}\\
 & = & \frac{W}{P}\left(1 + \frac{p}{\mathbb{P}}\right)
\end{array} 
$$

++++

Therefore as long as latexmath:[$\mathbb{P} \gg P$] (the parallelism is
much greater than the number of processors), then we obtain near
perfect speedup. (Speedup is latexmath:[$W/T_p$] and perfect speedup
would be latexmath:[$p$]).


The quantity latexmath:[$\mathbb{P}$], sometimes called *_average
parallelism_*, is usually quite large, because it usually grows
polynomially with the input size.

.Scheduler with a global thread queue.
=============

We can give a simple greedy scheduler by using a queue of threads.  At the
start of the execution, the scheduler places the root of the DAG into
the queue and then repeats the following step until the queue becomes
empty: for each idle processor, take the vertex at the front of the
queue and assign it to the processor, let each processor run for one
step, if at the end of the step, there is a vertex in the DAG whose
parents have all completed their execution, then insert that vertex at
the tail of the queue.

=============

.Centralized scheduler illustrated: the state of the queue and the DAG after step 4.  Completed vertices are drawn in grey (shaded).
[[centralized-scheduler, centralized scheduler illustrated]]
image::centralized-scheduler.png["Centralized scheduler illustrated",width="800pt",align="center"]



The centralized scheduler with the global thread queue is a greedy
scheduler that generates a greedy schedule under the assumption that
the queue operations take zero time and that the DAG is given. This
algorithm, however, does not work well for online scheduling the
operations on the queue take time.  In fact, since the thread queue is
global, the algorithm can only insert and remove one thread at a time.
For this reason, centralized schedulers do not scale beyond a handful
of processors.

.Definition: Scheduling friction.
************

No matter how efficient a scheduler is there is real cost to creating
threads, inserting and deleting them from queues, and to performing
load balancing.  We refer to these costs cumulatively as *_scheduling
friction_*, or simply as *_friction_*.

************


.Distributed scheduler illustrated: the state of the queue and the DAG after step 4.  Completed vertices are drawn in grey (shaded).
[[distributed-scheduler, distributed scheduler illustrated]]
image::distributed-scheduler.png["Distributed scheduler illustrated",width="800pt",align="center"]


There has been much research on the problem of reducing friction in
scheduling.  This research shows that distrubuted scheduling
algorithms can work quite well.  In a distributed algorithm, each
processor has its own queue and primarily operates on its own queue.
A load-balancing technique is then used to balance the load among the
existing processors by redistributing threads, usually on a needs
basis.  This strategy ensures that processors can operate in parallel
to obtain work from their queues.

A specific kind of distributed scheduling technique that can leads to
schedules that are close to optimal is *_work stealing_*
schedulers. In a work-stealing scheduler, processors work on their own
queues as long as their is work in them, and if not, go "steal" work
from other processors by removing the thread at the tail end of the
queue.  It has been proven that randomized work-stealing algorithm,
where idle processors randomly select processors to steal from,
deliver close to optimal schedules in expectation (in fact with high
probability) and furthermore incur minimal friction.  Randomized
schedulers can also be implemented efficiently in practice.  PASL uses
an scheduling algorithm that is based on work stealing.

