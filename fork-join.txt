[[ch:fork-join]]
Chapter: Fork-join parallelism
------------------------------

Fork-join parallelism, a fundamental model in parallel computing,
dates back to 1963 and has since been widely used in parallel
computing. In fork join parallelism, computations create opportunities
for parallelism by branching at certain points that are specified by
annotations in the program text.

Each branching point *_forks_* the control flow of the computation
into two or more logical threads. When control reaches the branching
point, the branches start running. When all branches complete, the
control *_joins_* back to unify the flows from the branches. Results
computed by the branches are typically read from memory and merged at
the join point. Parallel regions can fork and join recursively in the
same manner that divide and conquer programs split and join
recursively. In this sense, fork join is the divide and conquer of
parallel computing.


As we will see, it is often possible to extend an existing language
with support for fork-join parallelism by providing libraries or
compiler extensions that support a few simple primitives.  Such
extensions to a language make it easy to derive a sequential program
from a parallel program by syntactically substituting the parallelism
annotations with corresponding serial annotations.  This in turn
enables reasoning about the semantics or the meaning of parallel
programs by essentially "ignoring" parallelism. 


PASL is a Clatexmath:[++] library that enables writing implicitly parallel
programs.  In PASL, fork join is expressed by application of the
`fork2()` function. The function expects two arguments: one for each
of the two branches. Each branch is specified by one Clatexmath:[++]
lambda expression.

.Fork join
==========================

In the sample code below, the first branch writes the value 1 into the
cell `b1` and the second 2 into `b2`; at the join point, the sum of
the contents of `b1` and `b2` is written into the cell `j`.

[source,{cpp}]
----
long b1 = 0;
long b2 = 0;
long j  = 0;

fork2([&] {
  // first branch
  b1 = 1;
}, [&] {
  // second branch
  b2 = 2;
});
// join point
j = b1 + b2;

std::cout << "b1 = " << b1 << "; b2 = " << b2 << "; ";
std::cout << "j = " << j << ";" << std::endl;
----

Output:

----
b1 = 1; b2 = 2; j = 3;
----

==========================

When this code runs, the two branches of the fork join are both run to
completion. The branches may or may not run in parallel (i.e., on
different cores). In general, the choice of whether or not any two
such branches are run in parallel is chosen by the PASL runtime
system. The join point is scheduled to run by the PASL runtime only
after both branches complete. Before both branches complete, the join
point is effectively blocked. Later, we will explain in some more
detail the scheduling algorithms that the PASL uses to handle such
load balancing and synchronization duties.

In fork-join programs, a thread is a sequence of instructions that do
not contain calls to `fork2()`.  A thread is essentially a piece of
sequential computation.  The two branches passed to `fork2()` in the
example above correspond, for example, to two independent
threads. Moreover, the statement following the join point (i.e., the
continuation) is also a thread.

NOTE: If the syntax in the code above is unfamiliar, it might be a
good idea to review the notes on lambda expressions in Clatexmath:[++]11.  In a
nutshell, the two branches of `fork2()` are provided as
lambda-expressions where all free variables are passed by reference.

NOTE: Fork join of arbitrary arity is readily derived by repeated
application of binary fork join. As such, binary fork join is
universal because it is powerful enough to generalize to fork join of
arbitrary arity.

All writes performed by the branches of the binary fork join are
guaranteed by the PASL runtime to commit all of the changes that they
make to memory before the join statement runs. In terms of our code
snippet, all writes performed by two branches of `fork2` are committed
to memory before the join point is scheduled. The PASL runtime
guarantees this property by using a local barrier. Such barriers are
efficient, because they involve just a single dynamic synchronization
point between at most two processors.




.Writes and the join statement
==========================

In the example below, both writes into `b1` and `b2` are guaranteed to
be performed before the print statement.

[source,{cpp}]
----
long b1 = 0;
long b2 = 0;

fork2([&] {
  b1 = 1;
}, [&] {
  b2 = 2;
});

std::cout << "b1 = " << b1 << "; b2 = " << b2 << std::endl;
----

Output:
----
b1 = 1; b2 = 2
----

PASL provides no guarantee on the visibility of writes between any two
parallel branches. In the code just above, for example, writes
performed by the first branch (e.g., the write to `b1`) may or may not
be visible to the second, and vice versa. 

==========================


Parallel Fibonacci
~~~~~~~~~~~~~~~~~~

Now, we have all the tools we need to describe our first parallel
code: the recursive Fibonacci function. Although useless as a program
because of efficiency issues, this example is the "hello world" program
of parallel computing.

Recall that the latexmath:[$n^{th}$] Fibonnacci number is defined
by the recurrence relation

[latexmath]
++++
$$
\begin{array}{lcl}
F(n) & = & F(n-1) + F(n-2)
\end{array}
$$
++++

with base cases

[latexmath]
++++
$$
F(0) = 0, \, F(1) = 1
$$
++++

Let us start by considering a sequential algorithm. Following the
definition of Fibonacci numbers, we can write the code for
(inefficiently) computing the latexmath:[$n^{th}$] Fibonnacci number
as follows. This function for computing the Fibonacci numbers is
inefficient because the algorithm takes exponential time, whereas
there exist dynamic programming solutions that take linear time.


[source, {cpp}]
----
long fib_seq (long n) {
  long result;
  if (n < 2) {
    result = n;
  } else {
    long a, b;
    a = fib_seq(n-1);
    b = fib_seq(n-2);
    result = a + b;
  }
  return result;
}
----

To write a parallel version, we remark that the two recursive calls
are completely *_independent_*: they do not depend on each other
(neither uses a piece of data generated or written by another).  We
can therefore perform the recursive calls in parallel.  In general,
any two independent functions can be run in parallel.  To indicate
that two functions can be run in parallel, we use `fork2()`.

[source, {cpp}]
----
long fib_par(long n) {
  long result;
  if (n < 2) {
    result = n;
  } else {
    long a, b;
    fork2([&] {
      a = fib_par(n-1);
    }, [&] {
      b = fib_par(n-2);
    });
    result = a + b;
  }
  return result;
}
----


Incrementing an array, in parallel
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Suppose that we wish to map an array to another by incrementing each
element by one.  We can write the code for a function `map_incr` that
performs this computation serially.

[source,{cpp}]
----
void map_incr(const long* source, long* dest, long n) {
  for (long i = 0; i < n; i++)
    dest[i] = source[i] + 1;
}
----

.Example: Using `map_incr`.
=============
The code below illustrates  an example use of `map_incr`.

[source,{cpp}]
----
const long n = 4;
long xs[n] = { 1, 2, 3, 4 };
long ys[n];
map_incr(xs, ys, n);
for (long i = 0; i < n; i++)
  std::cout << ys[i] << " ";
std::cout << std::endl;
----

Output:

----
2 3 4 5 
----

=============

This is not a good parallel algorithm but it is not difficult to give
a parallel algorithm for incrementing an array. The code for such an
algorithm is given below.


[source,{cpp}]
----
void map_incr_rec(const long* source, long* dest, long lo, long hi) {
  long n = hi - lo;
  if (n == 0) {
    // do nothing
  } else if (n == 1) {
    dest[lo] = source[lo] + 1;
  } else {
    long mid = (lo + hi) / 2;
    fork2([&] {
      map_incr_rec(source, dest, lo, mid);
    }, [&] {
      map_incr_rec(source, dest, mid, hi);
    });
  }
}
----

It is easy to see that this algorithm has latexmath:[O(n)] work and
latexmath:[$O(\log{n})$] span.  


The sequential elision
~~~~~~~~~~~~~~~~~~~~~~

In the Fibonacci example, we started with a sequential algorithm and
derived a parallel algorithm by annotating independent functions.  It
is also possible to go the other way and derive a sequential algorithm
from a parallel one.  As you have probably guessed this direction is
easier, because all we have to do is remove the calls to the `fork2`
function. The sequential elision of our parallel Fibonacci code can be
written by replacing the call to `fork2()` with a statement that
performs the two calls (arguments of `fork2()`) sequentially as
follows.

[source, {cpp}]
----
long fib_par(long n) {
  long result;
  if (n < 2) {
    result = n;
  } else {
    long a, b;
    ([&] {
      a = fib_par(n-1);
    })();
    ([&] {
      b = fib_par(n-2);
    })();
    result = a + b;
  }
  return result;
}
----

NOTE: Although this code is slightly different than the sequential
version that we wrote, it is not too far away, because the only the
difference is the creation and application of the lambda-expressions.
An optimizing compiler for Clatexmath:[++] can easily "inline" such
computations. Indeed, After an optimizing compiler applies certain
optimizations, the performance of this code the same as the
performance of `fib_seq`.


The sequential elision is often useful for debugging and for
optimization.  It is useful for debugging because it is usually easier
to find bugs in sequential runs of parallel code than in parallel runs
of the same code.  It is useful in optimization because the
sequentialized code helps us to isolate the purely algorithmic
overheads that are introduced by parallelism. By isolating these
costs, we can more effectively pinpoint inefficiencies in our code.


