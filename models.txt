[[ch:models]]
== Models of Parallel Computation

[quote, James Christopher Wyllie, Ph.D. Thesis Cornell University 1979]
_____
Recent advances in microelectronics have brought closer to feasibility
the construction of computers containing thousands (or more)
processing elements.
_____

Parallel computing has fascinated computer scientists from the early
days of computing, even before parallel computers have become easily
available starting in 2000's. The architecture of early parallel
computers varied greatly. 

.  Cray-1 was the first vectorized parallel machine that can perform
operations on sequences of data called vectors.

.  ILLIAC IV had 64 processors laid out on a rectangular grid.  Each
processor had its own memory but could communicate with its four neighbors
on the grid and thus request data from them.  ILLIAC was a synchronous
machine where each processor would execute the same instruction in
each step, operating on its own memory.

. CM (Connection Machine) could have many (tens of thousands)
processors arranged into clusters, which are in turn arranged into
superclusters, and communication taking place through buses connecting
processors with each level of the cluster hierarchy.  Each processor
had its own memory and access the memory of others via the
communication bus.  The machine operated asynchronously allowing each
processor to perform instructions independently of the others.

This diversity continues to exist today.  For example, graphics
processors (GPUs), multicore computers, large data centers consisting
of many clusters of computers have characteristics of these earlier
designs.  

It is thus natural to consider the question of how one might design
algorithms for these machines.  This question may be viewed as
especially relevant because serial algorithms are traditionally
designed for the RAM (Random Access Memory) machine of computation,
which is equivalent to a Turing Machine and thus to Lambda Calculus.
In 1979, James C. Wyllie proposed the PRAM model as a RAM-like model
for parallel computing.  Wyllie viewed asynchronous computation as
inappropriate for the purposes of worst-case complexity analysis and
thus proposed a synchronous model of computation that combines the
synchronous computation model of ILLIAC-IV with the hierarchical
memory model of the Connection Machine.  As mentioned by Wyllie, the
PRAM model was used by many authors before it was proposed by Wyllie,
probably because it is a relatively natural generalization of the
sequential RAM model.

=== PRAM Model


==== The Machine
A PRAM consists of 

. an unbounded number of processors, latexmath:[$P_0, \ldots$],
. an unbounded global memory,
. a finite program.

Each processor in turn has 

. a unique processor id,
. a program counter,
. an unbounded local memory,
. a flag indicating whether or not a processor is running or active.

.A 4-Processor PRAM
====
The drawing below illustrates a 4-processor PRAM. 

image:jpgs-620H/pram-model.jpg["A 4-processor PRAM.",width="400pt",align="center"]
====

The PRAM model abstracts over various connection patterns used by
parallel architectures by supplying a global memory that is accessible
to all processors.

A PRAM program is a synchronous program that specifies the computation
performed by each processor at each step.  Execution of a PRAM program
proceeds in step.  In each step all active processors execute the
instruction pointed by their program counter.  The instruction may use
the id of the processor, which can be thought as being stored in the
local memory. For example, each processor latexmath:[$i$] can read the
latexmath:[$i^{th}$] cell of an array stored in global memory.  Each
processor can access its own local memory or the global memory but not
the local memory of another processor.  A processor may choose not to
participate in a step; such a processor would be inactive on that
step.  An active processor may activate an inactive processor and
direct it to a certain instruction by setting its program counter.

In his formulation of the PRAM model Wyllie did not permit multiple
processors to write into the same (global) memory cell.  Many
different variations of this model, however, been later proposed that
allow different degrees of "concurrency."  Some notable variants
include the following.

. *_EREW (Exclusive-Read-Exclusive-Write) PRAM:_* concurrent reads from
or writes into the same global memory cell are disallowed.

. *_CREW (Concurrent-Read-Exclusive-Write) PRAM:_* concurrent reads from
global memory cells are permitted but concurrent writes into the same
global memory cell are disallowed.

. *_CRCW (Concurrent-Read-Concurrent-Write) PRAM:_* concurrent reads from
and concurrent writes into the same global memory cells are
permitted.  It is possible to  distinguish further between different
CRCW PRAMs.

.. *_Common CRCW:_* concurrent writes must all write the same value.

.. *_Arbitrary CRCW:_* concurrent writes can write different values in a
step, but only one arbitrary write succeeds.

.. *_Priority CRCW:_ concurrent rites can write different values in a
step, but only the processor with the highest priority defined as the
processor with the minimum id succeeds.


In terms of computational power, these different models turn out to be
similar.

==== An Example: Array Sum

Suppose that we are given an array of elements stored in global memory
and wish to compute the sum of the elements.  We can write a PRAM
program to find the sum.  Suppose that the array contains
latexmath:[$n$] elements and we wish to use latexmath:[$n$]
processors.  Let's assume for simplicity that latexmath:[$n$] is a power of
latexmath:[$2$]. 

We can proceed in rounds.  In the first round, each processor
latexmath:[$P_i$], where latexmath:[$0 \le i  \le n/2$], adds up the pair of
elements at position latexmath:[$2i$] and latexmath:[$2i+1$] and places
it in position latexmath:[$i$].  In the second round, we repeat the
same computation but consider only the first latexmath:[$n/2$] of the
elements, and so on.  After latexmath:[$\lg{n}$] rounds, the sum is
placed in the first position of the input array.  The drawing below
illustrates this algorithm for an input of size latexmath:[$8$].

.Array sum in PRAM
====
The drawing below illustrates the PRAM array sum algorithm for an
array with latexmath:[$8$] elements. 

image:jpgs-620H/pram-array-sum.jpg["Array sum with PRAM.",width="400pt",align="center"]
====




We can write the code for this algorithm as follows. In the code the
local variable `i` denotes the id of the processor.  The computation
starts by each processor executing this piece of code.

.PRAM code for array sum
[source,cpp]
----
array_sum (A, n) = 
  for j = 1 to lg n  {
    active_procs  = n/2^j
    if (i < active_procs) {
      x = global_read A[2i]
      y = global_read A[2i+1]
      z = x + y
      global_write z into A[i] 
    }
----

Note the role that the synchronous execution of PRAM programs plays in
the example. If the processors did not execute each step synchronously
at the same time, then the execution can mix up results from different
rounds and obtain an incorrect result.

In this algorithm,  no (global) memory cell is read by more than one
processor at the same step.  Similarly, no (global) memory cell is
written my more than one processor at the same step.  This algorithm
is thus a EREW PRAM algorithm.



==== PRAM in Practice

Several assumptions of the PRAM model make it unlikely that the human
kind will ever be able to build an actual PRAM.

. Constant memory access: in PRAM, all processors can access memory in
constant time independent of the number of processors.  This is
currently impossible because an arbitrary number of processors and
memory cannot be packed into the same 3-dimensional space.  Assuming
that memory access speed is bounded by the speed of light, there will
thus be a dependence between the number of processors and the time for
memory access.

. Concurrent memory reads and writes: all known memory hardware can
serve a constant number of reads and writes in the same time step but
in a PRAM with latexmath:[$p$] processors, there can be
latexmath:[$p$] concurrent reads from and writes into the same memory
location.

Another problem with the PRAM model is that the PRAM algorithms do not
translate to practice well.

. The synchrony assumption, that all processors execute program
instructions in lock step, is nearly impossible to guarantee.  In
practice, parallel programs are executed by a system that maps many
processes on the same processors, swapping processes in and out as
needed.  Furthermore, the programmers write their programs using
higher level languages which then translate to many individual machine
instructions. Even if these instruction might be executed on the
hardware in lock step, the programmer does not have control over how
their high-level parallel programs are translated into low-level
instructions.

. PRAM programs specify instructions executed by each processor at
each time step.  In other words, they must specify the algorithm and
the schedule for the algorithm.  This is very tedious and extremely
difficult to do in practice because for example, we may not know the
number of processors available, or worse the number of processors may
change during execution.

For these reasons, the value of a PRAM algorithm from a practical
perspective is limited to the ideas of the algorithm.  Such ideas can
still be valuable but new ideas may be needed to implement a PRAM
algorithm in practice.

=== Work-Time Framework

Since a PRAM program must specify the action that each processor must
take at each step, it can be very tedious to use. Parallel algorithms
therefore are usually expressed in the *_work-time (WT) framework_*.
In this approach, we describe the parallel algorithm in terms of a
number of steps, where each step may contain any number of operations
that can all be executed in parallel.  To express such a step, all we
need is a "parallel for" construct, written `pfor`.  Once the
algorithm is described at this high level, we then use a general
scheduling principle to map the WT algorithm to a PRAM algorithm.  The
basic idea is to distribute the total work in each step among the
available processors as evenly as possible.  Since this second
transformation step is usually routine, and is often omitted.


.Array sum in work-time framework
====

The array-sum algorithm can be written as follows.

[source,cpp]
----
array_sum (A, n) = 
  for j = 1 to lg n  {
    pfor i = 0 to n/(2^j) {
      A[i] = A[2i] +  A[2i+1]
    }
  }
----

Having specified the algorithm, we can then derive a PRAM algorithm by
mapping each parallel step to a sequence of PRAM steps by distributing
the work among the available processors.  For example, if we wish to
use latexmath:[$n$] processors, then we can schedule `pfor` by
assigning each iteration to the first latexmath:[$\frac{n}{2^j}$]
processor.  Doing so yields the PRAM algorithm described earlier in
the section.
  
====

==== Work-Time Framework versus Work-Span Model

In this course, we primarily used the work-span model instead of the
work-time framework.  The two are similar but the work-span model is
somewhat more expressive: we don't have to use just parallel for's.
We can use any number of available parallelism constructs such as
fork-join, async-finish, and futures.  In other words, the work-span
model is more amenable as a language-level cost model.  In the
work-span model we also do not care to specify the PRAM algorithm at
all, because we expect that the algorithm will be executed with a
scheduler that is able to keep the processors busy without us
specifying how to do so.  As we have seen, this is a realistic
assumption, because there are scheduling algorithms such as work
stealing that can schedule computations efficiently.
