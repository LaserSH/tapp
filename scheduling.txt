Chapter: Executing parallel algorithms
--------------------------------------

Implicit parallelism allows writing parallel programs at a high level
of abstraction.  In this section, we discoss techniques for executing
such parallel programs on hardware-shared-memory computers such as
multicore computers. As our running example, we use the
`map_incr_rec`, whose code is reproduced below.

[source,{cpp}]
----
void map_incr_rec(const long* source, long* dest, long lo, long hi) {
  long n = hi - lo;
  if (n == 0) {
    // do nothing
  } else if (n == 1) {
    dest[lo] = source[lo] + 1;
  } else {
    long mid = (lo + hi) / 2;
    fork2([&] {
      map_incr_rec(source, dest, lo, mid);
    }, [&] {
      map_incr_rec(source, dest, mid, hi);
    });
  }
}
----

The basic idea is to partition a computation, that is a run of a
parallel algorithm on a specified input, into pieces of serial
computations, called threads, and map them to available processors
while observing the dependencies between them. The task of mapping the
threads to available processors is called *_thread scheduling_* or
simply *_scheduling_*.


We call a piece of serial computation a *_thread_*, if it executes
 without performing parallel operations (`fork2`) except perhaps as
 its last action.  The term thread is short for *_user-level thread_*
 (as opposed to a operating-system thread).  When partitioning the
 computation into threads, it is important for threads to be maximal
 to minimize scheduling overhead (technically a thread can be as small
 as a sincle instruction).

.Definition: Thread
***********

A *thread* is a maximal computation (execution of) consisting of a
sequence of instructions that do not contain calls to `fork2()` except
perhaps as its last action.

***********

When scheduling a parallel computation, it is important that we don't
alter the intended meaning of the computation.  Specifically, if a
thread depends another thread, because for example, it reads a piece
of data generated by the latter, it cannot be executed before the
thread that it depends on.  We can conservatively approximate such
dependencies by observing the `fork2` expressions and by organizing
dependencies consistently with them.  More specifically, we can
represent a computations as a graph where each vertex represents a
thread and each edge represents a dependency.  Vertices and edges are
created by execution of `fork2`.  Each `fork2` creates two threads
(vertices) corresponding to the two branches and inserts an edge
between each branch and the thread that performs the `fork2` branches;
in addition, each `fork2` creates a join or continuation thread
(vertex) that depends on the two branches. Since such a graph cannot
contain cycles, it is a *_Directed Acyclic Graph (DAG)_*.


.DAG for parallel increment on an array of latexmath:[$8$]: Each vertex corresponds a call to `map_inc_rec` excluding the `fork2`  or the continuation of `fork2`, which is empty, an is annotated with the interval of the input array that it operates on (its argument).
[[fig:parallel-inc-dag]]
image::inc-parallel-dag.png["DAG for parallel increment",width="800pt",align="center"]

Figure <<fig:parallel-inc-dag>> illustrates the DAG for an execution
of `map_incr_rec`.  We partition each invocation of this function into
two threads labeled by "M" and "C" respectively.  The threads labeled
by latexmath:[$M[i,j]$] corresponds to the part of the invocation of
`map_incr_rec` with arguments `lo` and `hi` set to latexmath:[$i$] and
latexmath:[$j$] respectively; this first part corresponds to the part
of execution up and including the `fork2` or all of the function if
this is a base case.  The second corresponds to the "continuation" of
the `fork2`, which is in this case includes no computation.

There is an important connection between computation DAG's and work
and span.  Suppose that we assign to each vertex a *_weight_* of at
least latexmath:[$1$] that corresponds to the work of that vertex
(since threads are serial work and span for each vertex is the same).
We can then calculate the total weight and total depth of the DAG by
summing up weights.  The total weight of the DAG corresponds to the
work of the computation and the depth corresponds to its span.  In our
example, each vertex has weight latexmath:[O(1)].  Thus for an array
with latexmath:[n] elements, the total weight (work) is
latexmath:[O(n)] and the depth (span) is latexmath:[$O(\log{n})$].

Having talked about DAG's we are now ready to talk about how to map
parallel computations to actual hardware so as to minimize their
run-time, i.e., scheduling.  But before we move on to scheduling let
us observe a few properties of implicitly parallel computations.

. The computation DAG of a parallel algorithm applied to an input
unfolds dynamically as the algorithm executes. For example, when we
run `map_inc_rec` with an input with latexmath:[$n$] elements, the DAG
initially contains just the root vertex (thread) corresponding to the
first call to `map_inc_rec` but it grows as the execution proceeds.

. An execution of a parallel algorithm can generate a massive number
of threads.  For example, our `map_inc_rec' function generates
approximately latexmath:[$4n$] threads for an input with
latexmath:[$n$] elements.

. The work/span of each thread can vary from a small amount to a very
large amount depending on the algorithm.  In our example, each thread
performs either a conditional, sometimes an addition and a fork
operation or performs no actual computation (continuation threads).

Suppose now we are given a computation DAG and we wish to execute the
DAG by mapping each thread to one of the latexmath:[$P$] processor that
is available on the hardware. When a thread is mapped to a processor,
it will be executed requiring time proportional to the work (weight)
of the thread.  Once the processor completes the thread, we can map
another thread to the processor, so that the processor does not idle
unnecessarily.  As we map threads to processors, it is important that
we observe the dependencies between threads, i.e., we don't execute a
thread before its parents. 

.Definition: Scheduling
************

The (thread) scheduling problem requires assigning to each thread in a
given DAG a processor number and a time step such that

. each thread is assigned to a unique processor for as many
consecutive steps as its weight,

. no thread is executed before its descendants in the DAG, and

. no processor is assigned more at most one thread at a time.

************

The goal of scheduling to minimize critical resources such as time.
Computing the shortest schedule for a DAG turns out to be highly
nontrivial. In fact, the related decision problem in NP-complete.  It
is possible, however, to give a good approximation algorithm for the
offline version of the problem to generate a 2-factor approximation.
Such an appraximation yields a schedule for a given DAG within a
factor-two of the shortest schedule.  In the online version of the
problem, where the DAG unfolds as the computation executes, we don't
know the DAG a priori and we have to account for the costs for
scheduling such as migrating threads between schedulers and finding
work.  To execute parallel programs, we need an solution to this
online version of the problem.

An online scheduler or a simply a *_scheduler_* is an algorithm that
performs scheduling by mapping threads to available processors.  For
example, if only one processor is available, a scheduler can map all
threads to that one processor.  If two processors are available, then
the scheduler can divide the threads between the two processors as
evenly as possible in an attempt to keep the two processors as busy
as possible by *_load balancing_*.

.An example 2-processor schedule
==========

The following is a valid schedule for the DAG shown in
<<fig:parallel-inc-dag, this Figure>> assuming that each thread takes
unit time.

[width="100%",frame="topbot",options="header"]
|==========
|Time Step | Processor 1               | Processor 2
|1         | M [0,8)                   | 
|2         | M [0,4)                   | M [4,8)
|3         | M [0,2)                   | M [4,6)
|4         | M [0,1)                   | M [4,5)
|5         | M [1,2)                   | M [5,6)
|6         | C [0,2)                   | C [4,6)
|7         | M [2,4)                   | M [6,8)
|8         | M [2,3)                   | M [6,7)
|9         | M [3,4)                   | M [7,8)
|10        | C [2,4)                   | C [6,8)
|11        | C [0,4)                   | C [4,8)
|12        | C [0,8)                   | _
|==========
==========



We say that a scheduler is *_greedy_* if, whenever there is a
processor available and a thread ready to be executed, then the
scheduler assigns the thread to the processor and starts running the
thread immediately.  Greedy schedulers have a nice property that is
summarized by the following theorem.


[[thm:greedy-scheduling]]
.Theorem: Greedy Scheduling Principle
***********

If a computation is run on latexmath:[$P$] processors using a perfect
greedy scheduler that incurs no costs in creating, locating, and
moving threads, then the total time (clock cycles) for running the
computation latexmath:[$T_P$] is bounded by

[latexmath]
++++

$$
T_P  <  \frac{W}{p} + S.
$$

++++

Here latexmath:[$W$] is the work of the computation, and
latexmath:[$S$] is the span of the computation (both measured in units
of clock cycles).

****


This simple statement is powerful.  To see this, note that the time to
execute the computation is at least latexmath:[$\frac{W}{P}$] because
we have a total of latexmath:[$W$] work. As such, the best possible
execution strategy is to divide it evenly among the processors.
Furthermore, execution time cannot be less than latexmath:[$S$] since
latexmath:[$S$] represents the longest chain of sequential
dependencies.  Thus we have: latexmath:[ $ T_P \geq
\max\left(\frac{W}{P},S\right).  $]

This means that a greedy schudeler yields a schedule that is within a
factor two of optimal: latexmath:[$\frac{W}{P} + S$] is never more
than twice latexmath:[$\max(\frac{W}{P},S)$]. Furthermore, when
latexmath:[$\frac{W}{P} \gg S$], the difference between the greedy
scheduler and the optimal scheduler is very small.  In fact, we can
rewrite equation above in terms of the average parallelism
latexmath:[$\mathbb{P} = W/S$] as follows:

[latexmath]
++++

$$
\begin{array}{rcl}
T_p & < & \frac{W}{P} + S \\
 & = & \frac{W}{P} + \frac{W}{\mathbb{P}}\\
 & = & \frac{W}{P}\left(1 + \frac{p}{\mathbb{P}}\right)
\end{array} 
$$

++++

Therefore as long as latexmath:[$\mathbb{P} \gg P$] (the parallelism is
much greater than the number of processors), then we obtain near
perfect speedup. (Speedup is latexmath:[$W/T_p$] and perfect speedup
would be latexmath:[$p$]).


The quantity latexmath:[$\mathbb{P}$], sometimes called *_average
parallelism_*, is usually quite large, because it usually grows
polynomially with the input size.

.Scheduler with a global thread queue.
=============

We can give a simple greedy scheduler by using a queue of threads.  At the
start of the execution, the scheduler places the root of the DAG into
the queue and then repeats the following step until the queue becomes
empty: for each idle processor, take the vertex at the front of the
queue and assign it to the processor, let each processor run for one
step, if at the end of the step, there is a vertex in the DAG whose
parents have all completed their execution, then insert that vertex at
the tail of the queue.

=============

.Centralized scheduler illustrated: the state of the queue and the DAG after step 4.  Completed vertices are drawn in grey (shaded).
[[centralized-scheduler, centralized scheduler illustrated]]
image::centralized-scheduler.png["Centralized scheduler illustrated",width="800pt",align="center"]



The centralized scheduler with the global thread queue is a greedy
scheduler that generates a greedy schedule under the assumption that
the queue operations take zero time and that the DAG is given. This
algorithm, however, does not work well for online scheduling the
operations on the queue take time.  In fact, since the thread queue is
global, the algorithm can only insert and remove one thread at a time.
For this reason, centralized schedulers do not scale beyond a handful
of processors.

.Definition: Scheduling friction.
************

No matter how efficient a scheduler is there is real cost to creating
threads, inserting and deleting them from queues, and to performing
load balancing.  We refer to these costs cumulatively as *_scheduling
friction_*, or simply as *_friction_*.

************


.Distributed scheduler illustrated: the state of the queue and the DAG after step 4.  Completed vertices are drawn in grey (shaded).
[[distributed-scheduler, distributed scheduler illustrated]]
image::distributed-scheduler.png["Distributed scheduler illustrated",width="800pt",align="center"]


There has been much research on the problem of reducing friction in
scheduling.  This research shows that distrubuted scheduling
algorithms can work quite well.  In a distributed algorithm, each
processor has its own queue and primarily operates on its own queue.
A load-balancing technique is then used to balance the load among the
existing processors by redistributing threads, usually on a needs
basis.  This strategy ensures that processors can operate in parallel
to obtain work from their queues.

A specific kind of distributed scheduling technique that can leads to
schedules that are close to optimal is *_work stealing_*
schedulers. In a work-stealing scheduler, processors work on their own
queues as long as their is work in them, and if not, go "steal" work
from other processors by removing the thread at the tail end of the
queue.  It has been proven that randomized work-stealing algorithm,
where idle processors randomly select processors to steal from,
deliver close to optimal schedules in expectation (in fact with high
probability) and furthermore incur minimal friction.  Randomized
schedulers can also be implemented efficiently in practice.  PASL uses
an scheduling algorithm that is based on work stealing.

